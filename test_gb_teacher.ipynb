{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "step 1:\n",
    "Train Good Teacher on retain data\n",
    "Train Bad Teacher on forget data\n",
    "Both teachers are frozen after training\n",
    "step 2:\n",
    "For each input:\n",
    "    Get predictions from Good Teacher\n",
    "    Get predictions from Bad Teacher\n",
    "    Get predictions from Student\n",
    "    Calculate divergence between Student and both Teachers\n",
    "    Update Student to move towards Good Teacher and away from Bad Teacher\n",
    "\n",
    "Total_Loss = α * KL(Student||Good_Teacher) - β * KL(Student||Bad_Teacher) + γ * Task_Loss\n",
    "\n",
    "1. Data Preparation\n",
    "\n",
    "2. Teacher Training:\n",
    "   Retain Data → Good Teacher\n",
    "   Forget Data → Bad Teacher\n",
    "\n",
    "3. Student Training:\n",
    "   Input → Student → Predictions\n",
    "         ↓\n",
    "   Good Teacher → KL Divergence\n",
    "         ↓\n",
    "   Bad Teacher  → KL Divergence\n",
    "         ↓\n",
    "   Loss Calculation → Model Update\n",
    "\n",
    "4. Evaluation:\n",
    "   Test Data → Models → Metrics → Logging\n",
    "'''\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from huggingface_hub import snapshot_download\n",
    "from utils import *\n",
    "\n",
    "\n",
    "class TeacherStudentUnlearning:\n",
    "    def __init__(self, good_teacher, bad_teacher, student, config):\n",
    "        self.good_teacher = good_teacher\n",
    "        self.bad_teacher = bad_teacher\n",
    "        self.student = student\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def train(self, dataloader, num_epochs):\n",
    "        self.good_teacher.to(self.device)\n",
    "        self.bad_teacher.to(self.device)\n",
    "        self.student.to(self.device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.student.parameters(), lr=self.config['lr'])\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "                loss = self.training_step(batch, optimizer)\n",
    "                total_loss += loss\n",
    "                \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            print(f\"Epoch {epoch+1} Average Loss: {avg_loss}\")\n",
    "            \n",
    "            if (epoch + 1) % self.config['training']['eval_frequency'] == 0:\n",
    "                self.evaluate()\n",
    "        \n",
    "    def training_step(self, batch, optimizer):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "        \n",
    "        # Get predictions from all models\n",
    "        with torch.no_grad():\n",
    "            good_teacher_output = self.good_teacher(**batch)\n",
    "            bad_teacher_output = self.bad_teacher(**batch)\n",
    "        \n",
    "        student_output = self.student(**batch)\n",
    "        \n",
    "        # Calculate losses\n",
    "        loss = self.calculate_combined_loss(\n",
    "            student_output.logits,\n",
    "            good_teacher_output.logits,\n",
    "            bad_teacher_output.logits,\n",
    "            batch\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.student.eval()\n",
    "        # todo: implement evaluation\n",
    "        pass\n",
    "\n",
    "    def save_checkpoint(self, path):\n",
    "        torch.save({\n",
    "            'student_state_dict': self.student.state_dict(),\n",
    "            'good_teacher_state_dict': self.good_teacher.state_dict(),\n",
    "            'bad_teacher_state_dict': self.bad_teacher.state_dict(),\n",
    "        }, path)\n",
    "        \n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.student.load_state_dict(checkpoint['student_state_dict'])\n",
    "        self.good_teacher.load_state_dict(checkpoint['good_teacher_state_dict'])\n",
    "        self.bad_teacher.load_state_dict(checkpoint['bad_teacher_state_dict'])\n",
    "    \n",
    "    def calculate_combined_loss(self, student_logits, good_teacher_logits, bad_teacher_logits, batch):\n",
    "        alpha = self.config['loss']['alpha']\n",
    "        beta = self.config['loss']['beta']\n",
    "        gamma = self.config['loss']['gamma']\n",
    "        \n",
    "        kl_good = self.kl_divergence(student_logits, good_teacher_logits)\n",
    "        kl_bad = self.kl_divergence(student_logits, bad_teacher_logits)\n",
    "        # todo: implement task loss calculation\n",
    "        task_loss = self.calculate_task_loss(student_logits, batch)\n",
    "        \n",
    "        return alpha * kl_good - beta * kl_bad + gamma * task_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def kl_divergence(student_logits, teacher_logits):\n",
    "        return torch.nn.functional.kl_div(\n",
    "            torch.nn.functional.log_softmax(student_logits, dim=-1),\n",
    "            torch.nn.functional.softmax(teacher_logits, dim=-1),\n",
    "            reduction='batchmean'\n",
    "        )\n",
    "    \n",
    "    def calculate_task_loss(self, student_logits, batch):\n",
    "        pass\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.base_path = \"/data1/malto/unlearning_llm\"\n",
    "        self.model_path = os.path.join(self.base_path, 'models/semeval25-unlearning-model-1B-model')\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.token = load_token()\n",
    "        \n",
    "        # Ensure model is downloaded\n",
    "        self._ensure_model_exists()\n",
    "    \n",
    "    def _ensure_model_exists(self):\n",
    "        \"\"\"Ensure the model exists locally, download if it doesn't\"\"\"\n",
    "        if not os.path.exists(self.model_path):\n",
    "            print(f\"Model not found at {self.model_path}, downloading...\")\n",
    "            snapshot_download(\n",
    "                repo_id='llmunlearningsemeval2025organization/olmo-1B-model-semeval25-unlearning',\n",
    "                token=self.token,\n",
    "                local_dir=self.model_path\n",
    "            )\n",
    "    \n",
    "    def initialize_teachers(self):\n",
    "        print(\"Initializing good teacher...\")\n",
    "        good_teacher = self._load_model()\n",
    "        \n",
    "        print(\"Initializing bad teacher...\")\n",
    "        bad_teacher = self._load_model()\n",
    "        \n",
    "        return good_teacher, bad_teacher\n",
    "\n",
    "    def initialize_student(self):\n",
    "        print(\"Initializing student...\")\n",
    "        return self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Helper method to load model with consistent parameters\"\"\"\n",
    "        try:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_path,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model from {self.model_path}\")\n",
    "            print(f\"Error details: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def freeze_teachers(self, good_teacher, bad_teacher):\n",
    "        print(\"Freezing teacher models...\")\n",
    "        for teacher in [good_teacher, bad_teacher]:\n",
    "            for param in teacher.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "class TeacherTrainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def train_good_teacher(self,teacher, dataloader):\n",
    "        teacher.to(self.device)\n",
    "        optimizer = torch.optim.Adam(teacher.parameters(), lr=self.config['training']['learning_rate'])\n",
    "        \n",
    "        for epoch in range(self.config['training']['num_epochs']):\n",
    "            total_loss = 0\n",
    "            for batch in tqdm(dataloader, desc=f\"Training Good Teacher - Epoch {epoch+1}\"):\n",
    "                loss = self.training_step(teacher, batch, optimizer)\n",
    "                total_loss += loss\n",
    "                \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            print(f\"Good Teacher Epoch {epoch+1} Average Loss: {avg_loss}\")\n",
    "\n",
    "\n",
    "    def train_bad_teacher(self, teacher, dataloader):\n",
    "        teacher.to(self.device)\n",
    "        optimizer = torch.optim.Adam(teacher.parameters(), lr=self.config['training']['learning_rate'])\n",
    "        \n",
    "        for epoch in range(self.config['training']['num_epochs']):\n",
    "            total_loss = 0\n",
    "            for batch in tqdm(dataloader, desc=f\"Training Bad Teacher - Epoch {epoch+1}\"):\n",
    "                loss = self.training_step(teacher, batch, optimizer)\n",
    "                total_loss += loss\n",
    "                \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            print(f\"Bad Teacher Epoch {epoch+1} Average Loss: {avg_loss}\")\n",
    "\n",
    "    def training_step(self, model, batch, optimizer):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def validate_teachers(self, good_teacher, bad_teacher, val_dataloader):\n",
    "        good_teacher.eval()\n",
    "        bad_teacher.eval()\n",
    "        # Implement validation metrics\n",
    "        pass\n",
    "\n",
    "class DataManager:\n",
    "    def __init__(self, config):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        self.retain_data = load_dataset(\"locuslab/TOFU\", 'retain')['train']\n",
    "        self.forget_data = load_dataset(\"locuslab/TOFU\", 'forget')['train']\n",
    "\n",
    "    def create_dataloaders(self, batch_size=8):\n",
    "        retain_dataset = UnlearningDataset(self.retain_data, self.tokenizer)\n",
    "        forget_dataset = UnlearningDataset(self.forget_data, self.tokenizer)\n",
    "        return DataLoader(retain_dataset, batch_size=batch_size, shuffle=True), DataLoader(forget_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "class UnlearningDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.questions = data['question']\n",
    "        self.answers = data['answer']\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = f\"{self.questions[idx]}{self.answers[idx]}\"\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "class ConfigManager:\n",
    "    def __init__(self):\n",
    "        # Base path from the server setup\n",
    "        base_path = \"/data1/malto/unlearning_llm\"\n",
    "        \n",
    "        self.config = {\n",
    "            'model': {\n",
    "                'path': f\"{base_path}/models/semeval25-unlearning-model-1B-model\",\n",
    "                'type': 'base_model',\n",
    "            },\n",
    "            'training': {\n",
    "                'batch_size': 1,\n",
    "                'learning_rate': 1e-5,\n",
    "                'num_epochs': 1,\n",
    "                'eval_frequency': 1,\n",
    "            },\n",
    "            'loss': {\n",
    "                'alpha': 1.0,  # Good Teacher weight\n",
    "                'beta': 0.5,   # Bad Teacher weight\n",
    "                'gamma': 1.0   # Task loss weight\n",
    "            },\n",
    "            'data': {\n",
    "                'base_path': f\"{base_path}/datasets/semeval25-unlearning-data\",\n",
    "                'max_length': 512\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def load_config(self, path):\n",
    "        with open(path, 'r') as f:\n",
    "            self.config = json.load(f)\n",
    "        \n",
    "    def save_config(self, path):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.config, f, indent=4)\n",
    "        \n",
    "    def update_config(self, updates):\n",
    "        for key, value in updates.items():\n",
    "            if isinstance(value, dict):\n",
    "                self.config[key].update(value)\n",
    "            else:\n",
    "                self.config[key] = value\n",
    "\n",
    "def main():\n",
    "    # Initialize configuration\n",
    "    config_manager = ConfigManager()\n",
    "    config = config_manager.config\n",
    "    \n",
    "    # Initialize components\n",
    "    model_manager = ModelManager(config)\n",
    "    data_manager = DataManager(config)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    retain_data, forget_data = data_manager.load_data()\n",
    "    retain_loader, forget_loader = data_manager.create_dataloaders(retain_data, forget_data)\n",
    "    \n",
    "    # Initialize models\n",
    "    good_teacher, bad_teacher = model_manager.initialize_teachers()\n",
    "    student = model_manager.initialize_student()\n",
    "    \n",
    "    # Train teachers\n",
    "    teacher_trainer = TeacherTrainer(config)\n",
    "    teacher_trainer.train_good_teacher(good_teacher, retain_loader)\n",
    "    teacher_trainer.train_bad_teacher(bad_teacher, forget_loader)\n",
    "    \n",
    "    # Freeze teachers\n",
    "    model_manager.freeze_teachers(good_teacher, bad_teacher)\n",
    "    \n",
    "    # Initialize unlearning system\n",
    "    unlearning = TeacherStudentUnlearning(good_teacher, bad_teacher, student, config)\n",
    "    \n",
    "    # Train student\n",
    "    unlearning.train(retain_loader, config['training']['num_epochs'])\n",
    "    \n",
    "    # Save final models\n",
    "    unlearning.save_checkpoint(\"final_checkpoint.pt\")\n",
    "    config_manager.save_config(\"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run():\n",
    "    import torch\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    try:\n",
    "        # Initialize configuration\n",
    "        logger.info(\"Initializing configuration...\")\n",
    "        config_manager = ConfigManager()\n",
    "        config = config_manager.config\n",
    "        \n",
    "        # Initialize model manager\n",
    "        logger.info(\"Initializing model manager...\")\n",
    "        model_manager = ModelManager(config)\n",
    "        \n",
    "        # Try loading just one model first as a test\n",
    "        logger.info(\"Testing model loading...\")\n",
    "        test_model = model_manager._load_model()\n",
    "        logger.info(\"Successfully loaded test model\")\n",
    "        \n",
    "        # If the test model loaded successfully, proceed with the rest\n",
    "        logger.info(\"Initializing data manager...\")\n",
    "        data_manager = DataManager(config)\n",
    "        \n",
    "        # Load and prepare data\n",
    "        logger.info(\"Loading data...\")\n",
    "        retain_loader, forget_loader = data_manager.create_dataloaders()\n",
    "        \n",
    "        logger.info(\"Data loading completed successfully!\")\n",
    "        \n",
    "        # Initialize all models\n",
    "        logger.info(\"Initializing all models...\")\n",
    "        good_teacher, bad_teacher = model_manager.initialize_teachers()\n",
    "        student = model_manager.initialize_student()\n",
    "        \n",
    "        logger.info(\"Models initialized successfully!\")\n",
    "        \n",
    "        # Continue with the rest of the process...\n",
    "        logger.info(\"Starting training process...\")\n",
    "        \n",
    "        # Train teachers\n",
    "        logger.info(\"Training teachers...\")\n",
    "        teacher_trainer = TeacherTrainer(config)\n",
    "        teacher_trainer.train_good_teacher(good_teacher, retain_loader)\n",
    "        teacher_trainer.train_bad_teacher(bad_teacher, forget_loader)\n",
    "        \n",
    "        # Freeze teachers\n",
    "        logger.info(\"Freezing teachers...\")\n",
    "        model_manager.freeze_teachers(good_teacher, bad_teacher)\n",
    "        \n",
    "        # Initialize unlearning system\n",
    "        logger.info(\"Initializing unlearning system...\")\n",
    "        unlearning = TeacherStudentUnlearning(good_teacher, bad_teacher, student, config)\n",
    "        \n",
    "        # Train student\n",
    "        logger.info(\"Training student...\")\n",
    "        unlearning.train(retain_loader, config['training']['num_epochs'])\n",
    "        \n",
    "        logger.info(\"Test completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error occurred: {str(e)}\")\n",
    "        logger.error(f\"Error type: {type(e)}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing configuration...\n",
      "INFO:__main__:Initializing model manager...\n",
      "INFO:__main__:Testing model loading...\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.53it/s]\n",
      "INFO:__main__:Successfully loaded test model\n",
      "INFO:__main__:Initializing data manager...\n",
      "ERROR:__main__:Error occurred: BuilderConfig 'retain' not found. Available: ['full', 'forget01', 'forget05', 'forget10', 'retain90', 'retain95', 'retain99', 'world_facts', 'real_authors', 'forget01_perturbed', 'forget05_perturbed', 'forget10_perturbed', 'retain_perturbed', 'world_facts_perturbed', 'real_authors_perturbed']\n",
      "ERROR:__main__:Error type: <class 'ValueError'>\n",
      "ERROR:__main__:Traceback: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2107970/1248022788.py\", line 24, in test_run\n",
      "    data_manager = DataManager(config)\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2107970/283428683.py\", line 248, in __init__\n",
      "    self.retain_data = load_dataset(\"locuslab/TOFU\", 'retain')['train']\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ebayat/Unlearning-sensitive-content-from-LLMs/.conda/lib/python3.11/site-packages/datasets/load.py\", line 2132, in load_dataset\n",
      "    builder_instance = load_dataset_builder(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ebayat/Unlearning-sensitive-content-from-LLMs/.conda/lib/python3.11/site-packages/datasets/load.py\", line 1890, in load_dataset_builder\n",
      "    builder_instance: DatasetBuilder = builder_cls(\n",
      "                                       ^^^^^^^^^^^^\n",
      "  File \"/home/ebayat/Unlearning-sensitive-content-from-LLMs/.conda/lib/python3.11/site-packages/datasets/builder.py\", line 342, in __init__\n",
      "    self.config, self.config_id = self._create_builder_config(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ebayat/Unlearning-sensitive-content-from-LLMs/.conda/lib/python3.11/site-packages/datasets/builder.py\", line 569, in _create_builder_config\n",
      "    raise ValueError(\n",
      "ValueError: BuilderConfig 'retain' not found. Available: ['full', 'forget01', 'forget05', 'forget10', 'retain90', 'retain95', 'retain99', 'world_facts', 'real_authors', 'forget01_perturbed', 'forget05_perturbed', 'forget10_perturbed', 'retain_perturbed', 'world_facts_perturbed', 'real_authors_perturbed']\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "BuilderConfig 'retain' not found. Available: ['full', 'forget01', 'forget05', 'forget10', 'retain90', 'retain95', 'retain99', 'world_facts', 'real_authors', 'forget01_perturbed', 'forget05_perturbed', 'forget10_perturbed', 'retain_perturbed', 'world_facts_perturbed', 'real_authors_perturbed']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 67\u001b[0m, in \u001b[0;36mtest_run\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[1;32m     66\u001b[0m logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraceback: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[19], line 24\u001b[0m, in \u001b[0;36mtest_run\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# If the test model loaded successfully, proceed with the rest\u001b[39;00m\n\u001b[1;32m     23\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing data manager...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m data_manager \u001b[38;5;241m=\u001b[39m \u001b[43mDataManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Load and prepare data\u001b[39;00m\n\u001b[1;32m     27\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 248\u001b[0m, in \u001b[0;36mDataManager.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretain_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocuslab/TOFU\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforget_data \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocuslab/TOFU\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforget\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Unlearning-sensitive-content-from-LLMs/.conda/lib/python3.11/site-packages/datasets/load.py:2132\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2127\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2128\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2129\u001b[0m )\n\u001b[1;32m   2131\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2132\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2147\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2149\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/Unlearning-sensitive-content-from-LLMs/.conda/lib/python3.11/site-packages/datasets/load.py:1890\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1888\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m get_dataset_builder_class(dataset_module, dataset_name\u001b[38;5;241m=\u001b[39mdataset_name)\n\u001b[1;32m   1889\u001b[0m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[0;32m-> 1890\u001b[0m builder_instance: DatasetBuilder \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1896\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1897\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1902\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1903\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1904\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39m_use_legacy_cache_dir_if_possible(dataset_module)\n\u001b[1;32m   1906\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[0;32m~/Unlearning-sensitive-content-from-LLMs/.conda/lib/python3.11/site-packages/datasets/builder.py:342\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, repo_id, data_files, data_dir, storage_options, writer_batch_size, **config_kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m     config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data_dir\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_kwargs \u001b[38;5;241m=\u001b[39m config_kwargs\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_builder_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# Prefill datasetinfo\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;66;03m# TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\u001b[39;00m\n",
      "File \u001b[0;32m~/Unlearning-sensitive-content-from-LLMs/.conda/lib/python3.11/site-packages/datasets/builder.py:569\u001b[0m, in \u001b[0;36mDatasetBuilder._create_builder_config\u001b[0;34m(self, config_name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m     builder_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_configs\u001b[38;5;241m.\u001b[39mget(config_name)\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS:\n\u001b[0;32m--> 569\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilderConfig \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found. Available: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_configs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m         )\n\u001b[1;32m    573\u001b[0m \u001b[38;5;66;03m# if not using an existing config, then create a new config on the fly\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m builder_config:\n",
      "\u001b[0;31mValueError\u001b[0m: BuilderConfig 'retain' not found. Available: ['full', 'forget01', 'forget05', 'forget10', 'retain90', 'retain95', 'retain99', 'world_facts', 'real_authors', 'forget01_perturbed', 'forget05_perturbed', 'forget10_perturbed', 'retain_perturbed', 'world_facts_perturbed', 'real_authors_perturbed']"
     ]
    }
   ],
   "source": [
    "test_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
