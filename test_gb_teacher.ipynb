{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebayat/Unlearning-sensitive-content-from-LLMs/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from good_bad_teacher import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run():\n",
    "    import torch\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    try:\n",
    "        # Initialize configuration\n",
    "        logger.info(\"Initializing configuration...\")\n",
    "        config_manager = ConfigManager()\n",
    "        config = config_manager.config\n",
    "        \n",
    "        # Initialize model manager\n",
    "        logger.info(\"Initializing model manager...\")\n",
    "        model_manager = ModelManager(config)\n",
    "        \n",
    "        # Try loading just one model first as a test\n",
    "        logger.info(\"Testing model loading...\")\n",
    "        test_model = model_manager._load_model()\n",
    "        logger.info(\"Successfully loaded test model\")\n",
    "        \n",
    "        # If the test model loaded successfully, proceed with the rest\n",
    "        logger.info(\"Initializing data manager...\")\n",
    "        data_manager = DataManager(config)\n",
    "        \n",
    "        # Load and prepare data\n",
    "        logger.info(\"Loading data...\")\n",
    "        retain_train_loader, forget_train_loader, retain_val_loader, forget_val_loader = data_manager.create_dataloaders(batch_size=8)\n",
    "        \n",
    "        logger.info(\"Data loading completed successfully!\")\n",
    "        \n",
    "        # Initialize all models\n",
    "        logger.info(\"Initializing all models...\")\n",
    "        good_teacher, bad_teacher = model_manager.initialize_teachers()\n",
    "        student = model_manager.initialize_student()\n",
    "        \n",
    "        logger.info(\"Models initialized successfully!\")\n",
    "        \n",
    "        logger.info(\"Test completed successfully!\")\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error occurred: {str(e)}\")\n",
    "        logger.error(f\"Error type: {type(e)}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing configuration...\n",
      "INFO:__main__:Initializing model manager...\n",
      "INFO:__main__:Testing model loading...\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  4.71it/s]\n",
      "INFO:__main__:Successfully loaded test model\n",
      "INFO:__main__:Initializing data manager...\n",
      "INFO:__main__:Loading data...\n",
      "INFO:__main__:Data loading completed successfully!\n",
      "INFO:__main__:Initializing all models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing good teacher...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing bad teacher...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing student...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  4.74it/s]\n",
      "INFO:__main__:Models initialized successfully!\n",
      "INFO:__main__:Test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "test_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing test components...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing good teacher...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  4.66it/s]\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing bad teacher...\n",
      "Initializing student...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  4.71it/s]\n",
      "INFO:__main__:Using device: cuda\n",
      "INFO:__main__:\n",
      "STEP 1: Testing forward pass...\n",
      "INFO:__main__:\n",
      "Good Teacher forward pass results:\n",
      "INFO:__main__:Output keys: odict_keys(['loss', 'logits', 'past_key_values'])\n",
      "INFO:__main__:Logits shape: torch.Size([2, 512, 50304])\n",
      "INFO:__main__:Loss value: 21.6610\n",
      "INFO:__main__:\n",
      "Bad Teacher forward pass results:\n",
      "INFO:__main__:Output keys: odict_keys(['loss', 'logits', 'past_key_values'])\n",
      "INFO:__main__:Logits shape: torch.Size([2, 512, 50304])\n",
      "INFO:__main__:Loss value: 24.7344\n",
      "INFO:__main__:\n",
      "Student forward pass results:\n",
      "INFO:__main__:Output keys: odict_keys(['loss', 'logits', 'past_key_values'])\n",
      "INFO:__main__:Logits shape: torch.Size([2, 512, 50304])\n",
      "INFO:__main__:Loss value: 21.6610\n",
      "INFO:__main__:‚úì Forward passes completed successfully\n",
      "INFO:__main__:\n",
      "STEP 2: Testing training step...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merfanbyt13\u001b[0m (\u001b[33merfanbyt13-politecnico-di-torino\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ebayat/Unlearning-sensitive-content-from-LLMs/wandb/run-20241219_145910-9b483jbc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/erfanbyt13-politecnico-di-torino/llm-unlearning/runs/9b483jbc' target=\"_blank\">likely-smoke-1</a></strong> to <a href='https://wandb.ai/erfanbyt13-politecnico-di-torino/llm-unlearning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/erfanbyt13-politecnico-di-torino/llm-unlearning' target=\"_blank\">https://wandb.ai/erfanbyt13-politecnico-di-torino/llm-unlearning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/erfanbyt13-politecnico-di-torino/llm-unlearning/runs/9b483jbc' target=\"_blank\">https://wandb.ai/erfanbyt13-politecnico-di-torino/llm-unlearning/runs/9b483jbc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Testing retain step:\n",
      "INFO:__main__:Initial loss: 21.8941\n",
      "INFO:__main__:Final loss: 21.5921\n",
      "INFO:__main__:Loss change: 0.3020\n",
      "INFO:__main__:\n",
      "Testing forget step:\n",
      "INFO:__main__:Initial loss: 20.1088\n",
      "INFO:__main__:Final loss: 20.4955\n",
      "INFO:__main__:Loss change: -0.3867\n",
      "INFO:__main__:‚úì Training steps completed successfully\n",
      "INFO:__main__:\n",
      "üéâ All tests completed successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Final Result:\n",
      "üéâ All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "import traceback\n",
    "from good_bad_teacher import (\n",
    "    ConfigManager, \n",
    "    DataManager, \n",
    "    ModelManager,\n",
    "    TeacherStudentUnlearning\n",
    ")\n",
    "\n",
    "def test_model_steps():\n",
    "    \"\"\"\n",
    "    Test single forward pass and training step for the Good-Bad Teacher implementation.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    try:\n",
    "        # Initialize components\n",
    "        logger.info(\"Initializing test components...\")\n",
    "        config_manager = ConfigManager()\n",
    "        config = config_manager.config\n",
    "        data_manager = DataManager(config)\n",
    "        model_manager = ModelManager(config)\n",
    "\n",
    "        # Get dataloaders\n",
    "        retain_loader, forget_loader, _, _ = data_manager.create_dataloaders(batch_size=2)\n",
    "        \n",
    "        # Initialize models\n",
    "        good_teacher, bad_teacher = model_manager.initialize_teachers()\n",
    "        student = model_manager.initialize_student()\n",
    "        \n",
    "        # Set up device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {device}\")\n",
    "        \n",
    "        #######################\n",
    "        # 1. Test Forward Pass\n",
    "        #######################\n",
    "        logger.info(\"\\nSTEP 1: Testing forward pass...\")\n",
    "        try:\n",
    "            # Move models to device\n",
    "            good_teacher = good_teacher.to(device)\n",
    "            bad_teacher = bad_teacher.to(device)\n",
    "            student = student.to(device)\n",
    "            \n",
    "            # Set models to eval mode\n",
    "            good_teacher.eval()\n",
    "            bad_teacher.eval()\n",
    "            student.eval()\n",
    "            \n",
    "            # Get a batch\n",
    "            batch = next(iter(retain_loader))\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Test forward pass for each model\n",
    "            with torch.no_grad():\n",
    "                # Good teacher forward pass\n",
    "                good_teacher_output = good_teacher(**batch)\n",
    "                logger.info(\"\\nGood Teacher forward pass results:\")\n",
    "                logger.info(f\"Output keys: {good_teacher_output.keys()}\")\n",
    "                logger.info(f\"Logits shape: {good_teacher_output.logits.shape}\")\n",
    "                logger.info(f\"Loss value: {good_teacher_output.loss.item():.4f}\")\n",
    "                \n",
    "                # Bad teacher forward pass\n",
    "                bad_teacher_output = bad_teacher(**batch)\n",
    "                logger.info(\"\\nBad Teacher forward pass results:\")\n",
    "                logger.info(f\"Output keys: {bad_teacher_output.keys()}\")\n",
    "                logger.info(f\"Logits shape: {bad_teacher_output.logits.shape}\")\n",
    "                logger.info(f\"Loss value: {bad_teacher_output.loss.item():.4f}\")\n",
    "                \n",
    "                # Student forward pass\n",
    "                student_output = student(**batch)\n",
    "                logger.info(\"\\nStudent forward pass results:\")\n",
    "                logger.info(f\"Output keys: {student_output.keys()}\")\n",
    "                logger.info(f\"Logits shape: {student_output.logits.shape}\")\n",
    "                logger.info(f\"Loss value: {student_output.loss.item():.4f}\")\n",
    "            \n",
    "            logger.info(\"‚úì Forward passes completed successfully\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(\"‚úó Error in forward pass testing:\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            raise\n",
    "\n",
    "        #######################\n",
    "        # 2. Test Training Step\n",
    "        #######################\n",
    "        logger.info(\"\\nSTEP 2: Testing training step...\")\n",
    "        try:\n",
    "            # Initialize unlearning system\n",
    "            unlearning_system = TeacherStudentUnlearning(\n",
    "                good_teacher=good_teacher,\n",
    "                bad_teacher=bad_teacher,\n",
    "                student=student,\n",
    "                config=config\n",
    "            )\n",
    "            \n",
    "            # Set student to training mode\n",
    "            student.train()\n",
    "            \n",
    "            # Keep teachers in eval mode\n",
    "            good_teacher.eval()\n",
    "            bad_teacher.eval()\n",
    "            \n",
    "            # Initialize optimizer\n",
    "            optimizer = torch.optim.Adam(\n",
    "                student.parameters(),\n",
    "                lr=config['training']['learning_rate']\n",
    "            )\n",
    "            \n",
    "            # Test retain step\n",
    "            logger.info(\"\\nTesting retain step:\")\n",
    "            batch = next(iter(retain_loader))\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Record initial loss\n",
    "            with torch.no_grad():\n",
    "                initial_output = student(**batch)\n",
    "                initial_loss = initial_output.loss.item()\n",
    "            \n",
    "            # Perform optimization step\n",
    "            optimizer.zero_grad()\n",
    "            retain_loss = unlearning_system.calculate_retain_loss(\n",
    "                student(**batch).logits,\n",
    "                good_teacher(**batch).logits,\n",
    "                batch\n",
    "            )\n",
    "            retain_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Record final loss\n",
    "            with torch.no_grad():\n",
    "                final_output = student(**batch)\n",
    "                final_loss = final_output.loss.item()\n",
    "            \n",
    "            logger.info(f\"Initial loss: {initial_loss:.4f}\")\n",
    "            logger.info(f\"Final loss: {final_loss:.4f}\")\n",
    "            logger.info(f\"Loss change: {initial_loss - final_loss:.4f}\")\n",
    "            \n",
    "            # Test forget step\n",
    "            logger.info(\"\\nTesting forget step:\")\n",
    "            batch = next(iter(forget_loader))\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Record initial loss\n",
    "            with torch.no_grad():\n",
    "                initial_output = student(**batch)\n",
    "                initial_loss = initial_output.loss.item()\n",
    "            \n",
    "            # Perform optimization step\n",
    "            optimizer.zero_grad()\n",
    "            forget_loss = unlearning_system.calculate_forget_loss(\n",
    "                student(**batch).logits,\n",
    "                bad_teacher(**batch).logits,\n",
    "                batch\n",
    "            )\n",
    "            forget_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Record final loss\n",
    "            with torch.no_grad():\n",
    "                final_output = student(**batch)\n",
    "                final_loss = final_output.loss.item()\n",
    "            \n",
    "            logger.info(f\"Initial loss: {initial_loss:.4f}\")\n",
    "            logger.info(f\"Final loss: {final_loss:.4f}\")\n",
    "            logger.info(f\"Loss change: {initial_loss - final_loss:.4f}\")\n",
    "            \n",
    "            logger.info(\"‚úì Training steps completed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(\"‚úó Error in training step testing:\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            raise\n",
    "\n",
    "        logger.info(\"\\nüéâ All tests completed successfully!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"\\n‚ùå Testing failed with error:\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = test_model_steps()\n",
    "    print(\"\\n‚ú® Final Result:\")\n",
    "    print(\"üéâ All tests passed successfully!\" if success else \"‚ùå Tests failed. Check the logs above for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 12.75it/s]\n"
     ]
    }
   ],
   "source": [
    "config = ConfigManager()\n",
    "m = config.config['model']['good_teacher']['path']\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing direct tokenizer load from: /data1/malto/unlearning_llm/models/semeval25-unlearning-model-1B-model\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for '/data1/malto/unlearning_llm/models/semeval25-unlearning-model-1B-model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/data1/malto/unlearning_llm/models/semeval25-unlearning-model-1B-model' is the correct path to a directory containing all relevant files for a GPTNeoXTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m test_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data1/malto/unlearning_llm/models/semeval25-unlearning-model-1B-model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting direct tokenizer load from:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_path)\n\u001b[0;32m----> 6\u001b[0m test_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Unlearning-sensitive-content-from-LLMs/.conda/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:939\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    936\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 939\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Unlearning-sensitive-content-from-LLMs/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2197\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2198\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2202\u001b[0m     )\n\u001b[1;32m   2204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for '/data1/malto/unlearning_llm/models/semeval25-unlearning-model-1B-model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/data1/malto/unlearning_llm/models/semeval25-unlearning-model-1B-model' is the correct path to a directory containing all relevant files for a GPTNeoXTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "# At the start of your script, test this:\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "test_path = \"/data1/malto/unlearning_llm/models/semeval25-unlearning-model-1B-model\"\n",
    "print(\"Testing direct tokenizer load from:\", test_path)\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(test_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /data1/malto/unlearning_llm/models/semeval25-unlearning-model-1B-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  8.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "path = \"/data1/malto/unlearning_llm/\"\n",
    "\n",
    "## Fetch and load model:\n",
    "model_path = path + 'models/semeval25-unlearning-model-1B-model'\n",
    "print(f\"Loading model from {model_path}\")\n",
    "#snapshot_download(repo_id='llmunlearningsemeval2025organization/olmo-finetuned-semeval25-unlearning', token=hf_token, local_dir=model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_ids: torch.Size([2, 512])\n",
      "Shape of attention_mask: torch.Size([2, 512])\n",
      "Shape of labels: torch.Size([2, 512])\n",
      "Sequence lengths in 'input_ids':\n",
      "Sequence 1: Length = 15\n",
      "Sequence 2: Length = 73\n"
     ]
    }
   ],
   "source": [
    "config_manager = ConfigManager()\n",
    "config = config_manager.config\n",
    "data_manager = DataManager(config)\n",
    "retain_loader, forget_loader, _, _ = data_manager.create_dataloaders(batch_size=2)\n",
    "len(retain_loader)\n",
    "first_batch = next(iter(retain_loader))\n",
    "print(\"Shape of input_ids:\", first_batch['input_ids'].shape)\n",
    "print(\"Shape of attention_mask:\", first_batch['attention_mask'].shape)\n",
    "print(\"Shape of labels:\", first_batch['labels'].shape)\n",
    "\n",
    "# Assuming 'first_batch' is the batch from your DataLoader\n",
    "input_ids = first_batch['input_ids']  # Tensor of input_ids\n",
    "\n",
    "# Print out sequence lengths for each item in the batch\n",
    "print(\"Sequence lengths in 'input_ids':\")\n",
    "for idx, seq in enumerate(input_ids):\n",
    "    seq_length = (seq != 1).sum().item()  # Count non-padding tokens (assuming padding token=1)\n",
    "    print(f\"Sequence {idx + 1}: Length = {seq_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from good_bad_teacher import (\n",
    "    ConfigManager, \n",
    "    DataManager, \n",
    "    ModelManager,\n",
    "    TeacherStudentUnlearning\n",
    ")\n",
    "import torch\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def setup_logging(output_dir):\n",
    "    \"\"\"Set up logging configuration\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    log_file = os.path.join(output_dir, f'training_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "def train_and_evaluate():\n",
    "    # Initialize configuration\n",
    "    config_manager = ConfigManager()\n",
    "    config = config_manager.config\n",
    "    \n",
    "    # Set up output directory and logging\n",
    "    output_dir = os.path.join(config['checkpoints_dir'], datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    logger = setup_logging(output_dir)\n",
    "    \n",
    "    try:\n",
    "        # Initialize components\n",
    "        logger.info(\"Initializing components...\")\n",
    "        data_manager = DataManager(config)\n",
    "        model_manager = ModelManager(config)\n",
    "        \n",
    "        # Load data\n",
    "        logger.info(\"Loading data...\")\n",
    "        retain_train_loader, forget_train_loader, retain_val_loader, forget_val_loader = \\\n",
    "            data_manager.create_dataloaders(batch_size=config['training'].get('batch_size', 8))\n",
    "        \n",
    "        # Initialize models\n",
    "        logger.info(\"Initializing models...\")\n",
    "        good_teacher, bad_teacher = model_manager.initialize_teachers()\n",
    "        student = model_manager.initialize_student()\n",
    "        \n",
    "        # Freeze teachers\n",
    "        logger.info(\"Freezing teacher models...\")\n",
    "        model_manager.freeze_teachers(good_teacher, bad_teacher)\n",
    "        \n",
    "        # Initialize unlearning system\n",
    "        logger.info(\"Setting up unlearning system...\")\n",
    "        unlearning_system = TeacherStudentUnlearning(\n",
    "            good_teacher=good_teacher,\n",
    "            bad_teacher=bad_teacher,\n",
    "            student=student,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        # Training\n",
    "        logger.info(\"Starting training...\")\n",
    "        training_history = unlearning_system.train_student(\n",
    "            retain_loader=retain_train_loader,\n",
    "            forget_loader=forget_train_loader,\n",
    "            validation_loader=retain_val_loader,\n",
    "            num_epochs=config['training']['num_epochs']\n",
    "        )\n",
    "        \n",
    "        # Final evaluation\n",
    "        logger.info(\"Performing final evaluation...\")\n",
    "        final_results = unlearning_system.evaluate(\n",
    "            retain_loader=retain_val_loader,\n",
    "            forget_loader=forget_val_loader\n",
    "        )\n",
    "        \n",
    "        # Save final model and results\n",
    "        final_checkpoint_path = os.path.join(output_dir, 'final_model.pt')\n",
    "        logger.info(f\"Saving final model to {final_checkpoint_path}\")\n",
    "        unlearning_system.save_checkpoint(\n",
    "            path=final_checkpoint_path,\n",
    "            epoch=config['training']['num_epochs'],\n",
    "            val_loss=training_history['val_losses'][-1],\n",
    "            metrics=final_results\n",
    "        )\n",
    "        \n",
    "        return True, training_history, final_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed with error: {str(e)}\")\n",
    "        logger.error(f\"Error type: {type(e)}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return False, None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success, history, results = train_and_evaluate()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n‚ú® Training completed successfully!\")\n",
    "        print(\"\\nTraining History:\")\n",
    "        for key, value in history.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        print(\"\\nFinal Results:\")\n",
    "        for key, value in results.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Training failed. Check the logs for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'good_teacher': {'path': '/data1/malto/unlearning_llm/models/semeval25-unlearning-model-1B-model',\n",
       "   'type': 'base_model'},\n",
       "  'bad_teacher': {'model_id': 'EleutherAI/pythia-70m',\n",
       "   'torch_dtype': 'float16'}},\n",
       " 'training': {'num_epochs': 10,\n",
       "  'learning_rate': 1e-05,\n",
       "  'checkpoint_frequency': 5,\n",
       "  'validation_frequency': 1},\n",
       " 'checkpoints_dir': '/path/to/checkpoints',\n",
       " 'validation': {'batch_size': 32,\n",
       "  'metrics': ['perplexity', 'agreement', 'divergence']},\n",
       " 'retain': {'alpha': 1.0, 'gamma': 0},\n",
       " 'forget': {'beta': 0.5, 'gamma': 0},\n",
       " 'loss': {'alpha': 1.0, 'beta': 0.5, 'gamma': 0},\n",
       " 'data': {'base_path': '/data1/malto/unlearning_llm/datasets/semeval25-unlearning-data',\n",
       "  'max_length': 512}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_manager = ConfigManager()\n",
    "config = config_manager.config\n",
    "config[\"loss\"][\"gamma\"] = 0\n",
    "config[\"retain\"][\"gamma\"] = 0\n",
    "config[\"forget\"][\"gamma\"] = 0\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merfanbyt13\u001b[0m (\u001b[33merfanbyt13-politecnico-di-torino\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ebayat/Unlearning-sensitive-content-from-LLMs/wandb/run-20241219_152126-wm6f9ot5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/erfanbyt13-politecnico-di-torino/llm-unlearning/runs/wm6f9ot5' target=\"_blank\">polished-aardvark-3</a></strong> to <a href='https://wandb.ai/erfanbyt13-politecnico-di-torino/llm-unlearning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/erfanbyt13-politecnico-di-torino/llm-unlearning' target=\"_blank\">https://wandb.ai/erfanbyt13-politecnico-di-torino/llm-unlearning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/erfanbyt13-politecnico-di-torino/llm-unlearning/runs/wm6f9ot5' target=\"_blank\">https://wandb.ai/erfanbyt13-politecnico-di-torino/llm-unlearning/runs/wm6f9ot5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing good teacher...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing bad teacher...\n",
      "Initializing student...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing teacher models...\n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Retain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 142/142 [01:59<00:00,  1.19it/s]\n",
      "Training Forget: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139/139 [01:33<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation at epoch 1\n",
      "Validation Loss: 8.0211\n",
      "Validation Metrics: {'perplexity': 3068.4190220424107, 'good_teacher_agreement': 0.11278599330357143, 'bad_teacher_divergence': 0.42303757497242517}\n",
      "Retain Loss: 2038.6643\n",
      "Forget Loss: 418.2347\n",
      "Validation Loss: 8.0211\n",
      "Validation Metrics: {'perplexity': 3068.4190220424107, 'good_teacher_agreement': 0.11278599330357143, 'bad_teacher_divergence': 0.42303757497242517}\n",
      "\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Retain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 142/142 [02:04<00:00,  1.14it/s]\n",
      "Training Forget: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139/139 [01:35<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation at epoch 2\n",
      "Validation Loss: 7.9593\n",
      "Validation Metrics: {'perplexity': 2886.243540736607, 'good_teacher_agreement': 0.11278599330357143, 'bad_teacher_divergence': 0.42303757497242517}\n",
      "Retain Loss: 1821.0704\n",
      "Forget Loss: 397.6603\n",
      "Validation Loss: 7.9593\n",
      "Validation Metrics: {'perplexity': 2886.243540736607, 'good_teacher_agreement': 0.11278599330357143, 'bad_teacher_divergence': 0.42303757497242517}\n",
      "\n",
      "Evaluating on retain data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retain Evaluation:   0%|          | 0/35 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'max_new_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Clean up wandb\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     unlearning\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[0;32m---> 63\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m training_history \u001b[38;5;241m=\u001b[39m unlearning\u001b[38;5;241m.\u001b[39mtrain_student(retain_train_loader, forget_train_loader, retain_val_loader, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Final evaluation\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m final_results \u001b[38;5;241m=\u001b[39m \u001b[43munlearning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_val_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforget_val_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Log final results summary\u001b[39;00m\n\u001b[1;32m     35\u001b[0m wandb\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_retain_regurgitation\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(final_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregurgitation-score\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_retain_knowledge\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(final_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknowledge-score\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_validation_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, training_history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_losses\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     42\u001b[0m })\n",
      "File \u001b[0;32m~/Unlearning-sensitive-content-from-LLMs/good_bad_teacher.py:308\u001b[0m, in \u001b[0;36mTeacherStudentUnlearning.evaluate\u001b[0;34m(self, retain_loader, forget_loader)\u001b[0m\n\u001b[1;32m    305\u001b[0m         good_teacher_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgood_teacher(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudent_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgood_teacher_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# Evaluate on forget data\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating on forget data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Unlearning-sensitive-content-from-LLMs/good_bad_teacher.py:375\u001b[0m, in \u001b[0;36mTeacherStudentUnlearning._calculate_metrics\u001b[0;34m(self, student_output, teacher_output, batch, scorer, results_dict)\u001b[0m\n\u001b[1;32m    369\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(student_output\u001b[38;5;241m.\u001b[39mloss)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# Generate text for ROUGE and knowledge score\u001b[39;00m\n\u001b[1;32m    372\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudent\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    373\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    374\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m--> 375\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_new_tokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    376\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    377\u001b[0m )\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# Decode outputs\u001b[39;00m\n\u001b[1;32m    380\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[1;32m    381\u001b[0m     generated_ids[:, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:],\n\u001b[1;32m    382\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    384\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'max_new_tokens'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Initialize configuration\n",
    "    config_manager = ConfigManager()\n",
    "    config = config_manager.config\n",
    "    config[\"loss\"][\"gamma\"] = 0\n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=\"llm-unlearning\",\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    model_manager = ModelManager(config)\n",
    "    data_manager = DataManager(config)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    retain_train_loader, forget_train_loader, retain_val_loader, forget_val_loader = data_manager.create_dataloaders(batch_size=8)\n",
    "    \n",
    "    # Initialize models\n",
    "    good_teacher, bad_teacher = model_manager.initialize_teachers()\n",
    "    student = model_manager.initialize_student()\n",
    "    \n",
    "    # Freeze teachers\n",
    "    model_manager.freeze_teachers(good_teacher, bad_teacher)\n",
    "    \n",
    "    # Initialize unlearning system\n",
    "    unlearning = TeacherStudentUnlearning(good_teacher, bad_teacher, student, config)\n",
    "    \n",
    "    # Train student\n",
    "    training_history = unlearning.train_student(retain_train_loader, forget_train_loader, retain_val_loader, config['training']['num_epochs'])\n",
    "    \n",
    "    # Final evaluation\n",
    "    final_results = unlearning.evaluate(retain_val_loader, forget_val_loader)\n",
    "    \n",
    "    # Log final results summary\n",
    "    wandb.run.summary.update({\n",
    "        \"final_retain_regurgitation\": np.mean(final_results['retain']['regurgitation-score']),\n",
    "        \"final_retain_knowledge\": np.mean(final_results['retain']['knowledge-score']),\n",
    "        \"final_forget_regurgitation\": np.mean(final_results['forget']['regurgitation-score']),\n",
    "        \"final_forget_knowledge\": np.mean(final_results['forget']['knowledge-score']),\n",
    "        \"training_epochs\": config['training']['num_epochs'],\n",
    "        \"best_validation_loss\": min(filter(None, training_history['val_losses']))\n",
    "    })\n",
    "\n",
    "    # Save final results to a file\n",
    "    with open(os.path.join(config['checkpoints_dir'], 'final_results.json'), 'w') as f:\n",
    "        json.dump(final_results, f, indent=4)\n",
    "        \n",
    "    # Log final results file as artifact\n",
    "    final_results_artifact = wandb.Artifact(\n",
    "        \"final_results\", \n",
    "        type=\"results\",\n",
    "        description=\"Final evaluation results\"\n",
    "    )\n",
    "    final_results_artifact.add_file(os.path.join(config['checkpoints_dir'], 'final_results.json'))\n",
    "    wandb.log_artifact(final_results_artifact)\n",
    "    \n",
    "    # Save config\n",
    "    config_manager.save_config(\"config.json\")\n",
    "    \n",
    "    # Clean up wandb\n",
    "    unlearning.finish()\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
